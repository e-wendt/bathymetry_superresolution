{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b21d9b-acb1-42d9-a889-dd6064126501",
   "metadata": {},
   "source": [
    "# Training Super-Resolution Models \n",
    "## (for Bathymetry resolution enhancement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a234a69-c589-4f8a-bf0d-1b38e01179df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os \n",
    "import pathlib\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a4446-7257-43cd-9629-5037b5959f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba511e70-54b4-47b1-9330-7e7dc197e744",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Datasets and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57208f-5578-4586-8cf3-cdffd1485839",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'GEBCO_x2' :    {'HR_train' : 'datasets/GEBCO/x2/train/HR',\n",
    "                             'LR_train' : 'datasets/GEBCO/x2/train/LR',\n",
    "                             'HR_test' : 'datasets/GEBCO/x2/test/HR',\n",
    "                             'LR_test' : 'datasets/GEBCO/x2/test/LR',\n",
    "                            }, \n",
    "            \n",
    "            'GEBCO_x4' :    {'HR_train' : 'datasets/GEBCO/x4/train/HR',\n",
    "                             'LR_train' : 'datasets/GEBCO/x4/train/LR',\n",
    "                             'HR_test' : 'datasets/GEBCO/x4/test/HR',\n",
    "                             'LR_test' : 'datasets/GEBCO/x4/test/LR',\n",
    "                            }, \n",
    "            \n",
    "            'Pangaea_256' : {'HR_train' : 'datasets/Pangaea/256/train/HR',\n",
    "                             'HR_test' : 'datasets/Pangaea/256/test/HR',\n",
    "                            }, \n",
    "            \n",
    "            'Pangaea_128' : {'HR_train' : 'datasets/Pangaea/128/train/HR',\n",
    "                             'HR_test' : 'datasets/Pangaea/128/test/HR',\n",
    "                            }, \n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2b2e6-3223-47e0-a777-3ae946f552b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = 'GEBCO_x2'\n",
    "#name = 'GEBCO_x4'\n",
    "name = 'Pangaea_256'\n",
    "#name = 'Pangaea_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12800eed-8ab3-4b9f-80a0-54ac3844292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\n",
    "    'batch_size' : 8,                   # Number of image samples used in each training step          \n",
    "    'hr_dimension' : 256,               # Dimension of a High Resolution (HR) Image\n",
    "    'scale' : 2,                        # Factor by which Low Resolution (LR) Images will be downscaled.\n",
    "    'data_name': name,                  # Dataset name\n",
    "    'trunk_size' : 23,                  # Number of Residual blocks used in Generator,\n",
    "    'init_lr' : 0.0001,                 # Initial Learning rate for generator. \n",
    "    'disc_init_lr' : 0.00005,           # Initial learning rate for discriminator.\n",
    "    'ph2_steps' : 50000,                # Number of steps required for phase-2 training\n",
    "    'decay_ph2' : 0.5,                  # Factor by which learning rates are modified during phase-2 training    \n",
    "    'lambda' : 0.005,                   # To balance adversarial loss during phase-2 training. \n",
    "    'eta' : 0.01,                       # To balance L1 loss during phase-2 training.    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa8d40-e750-492f-997e-8656cac3e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'models/GAN_Pangaea/Generator_'+ Params['data_name'] +'_x' + str(Params['scale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86613fa5-fa3b-407b-a7b9-cee7d80d6e58",
   "metadata": {},
   "source": [
    "----- \n",
    "\n",
    "#### Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7f304-d82b-4127-bebf-32e438d8f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize values from [min, max] to [0-1]\n",
    "def normalize(image, idx): \n",
    "    min_val = min_vals[idx]\n",
    "    max_val = max_vals[idx]\n",
    "    return (image - (min_val)) / (max_val - min_val)\n",
    "\n",
    "# normalize values from [min, max] to [0-1]\n",
    "def normalize_test(image, idx): \n",
    "    min_val = test_min_vals[idx]\n",
    "    max_val = test_max_vals[idx]\n",
    "    return (image - (min_val)) / (max_val - min_val)\n",
    "\n",
    "def load_dataset_with_degraded_LR(HR_file_path, is_val_ds, remove=False): \n",
    "\n",
    "    dataset_name = Params['data_name']\n",
    "    img_dir=dataset[dataset_name][HR_file_path]\n",
    "    all_files=os.listdir(img_dir)\n",
    "    hr = [os.path.join(img_dir + \"/\" + i) for i in all_files]\n",
    "    print('HR Training Samples: ', len(hr), 'Files found')\n",
    "    \n",
    "    hr_ds = [np.load(x) for x in hr]\n",
    "    \n",
    "    if is_val_ds == True: \n",
    "        global test_min_vals\n",
    "        global test_max_vals\n",
    "        test_min_vals = [x.min() for x in hr_ds]\n",
    "        test_max_vals = [x.max() for x in hr_ds]\n",
    "        hr_ds = [normalize_test(x, idx) for idx, x in enumerate(hr_ds)]\n",
    "        \n",
    "    else: \n",
    "        global min_vals\n",
    "        global max_vals\n",
    "        # get min and max values of each sample pair\n",
    "        min_vals = [x.min() for x in hr_ds]\n",
    "        max_vals = [x.max() for x in hr_ds]\n",
    "        hr_ds = [normalize(x, idx) for idx, x in enumerate(hr_ds)]\n",
    "        \n",
    "        \n",
    "    hr_ds = tf.cast(hr_ds, tf.float32)\n",
    "    hr_ds = [tf.stack((x,)*3, axis=-1) for x in hr_ds]\n",
    "    hr_ds = tf.convert_to_tensor(hr_ds)\n",
    "    \n",
    "    lr_degraded = [(tf.image.resize(tf.squeeze(x), \n",
    "                             [hr_ds[0].shape[0] // Params['scale'],hr_ds[0].shape[0] // Params['scale']], \n",
    "                             method=tf.image.ResizeMethod.BICUBIC) )\n",
    "                    for x in hr_ds]\n",
    "    \n",
    "    lr_ds = tf.convert_to_tensor(lr_degraded)\n",
    "    \n",
    "    print(\"LR Dataset Shape: \", lr_ds.shape)\n",
    "    print(\"HR Dataset Shape: \", hr_ds.shape)\n",
    "    \n",
    "    return lr_ds, hr_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9757b9-e710-44d4-92fa-da88fd5f9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if LR file path == None than do degraded -- else use LR path with real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585d0c7-2b9b-459e-bd27-ee852c3a6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_real_LR(HR_file_path, LR_file_path, is_val_ds, remove=False):\n",
    "    \n",
    "    img_dir=Params['data_name'][HR_file_path]\n",
    "    all_files=os.listdir(img_dir)\n",
    "    hr = [os.path.join(img_dir + \"/\" + i) for i in all_files]\n",
    "    print('HR Training Samples: ', len(hr), 'Files found')\n",
    "    \n",
    "    hr_ds = [np.load(x) for x in hr]\n",
    "    \n",
    "    img_dir=Params['data_name'][LR_file_path]\n",
    "    all_files=os.listdir(img_dir)\n",
    "    lr = [os.path.join(img_dir + \"/\" + i) for i in all_files]\n",
    "    print('LR Training Samples: ', len(lr), 'Files found')\n",
    "    \n",
    "    lr_ds = [np.load(x) for x in lr]\n",
    "    \n",
    "    if is_val_ds == True: \n",
    "        global test_min_vals\n",
    "        global test_max_vals\n",
    "        test_min_vals = [x.min() for x in hr_ds]\n",
    "        test_max_vals = [x.max() for x in hr_ds]\n",
    "        hr_ds = [normalize_test(x, idx) for idx, x in enumerate(hr_ds)]\n",
    "        lr_ds = [normalize_test(x, idx) for idx, x in enumerate(lr_ds)]\n",
    "        \n",
    "    else: \n",
    "        global min_vals\n",
    "        global max_vals\n",
    "        # get min and max values of each sample pair\n",
    "        min_vals = [x.min() for x in hr_ds]\n",
    "        max_vals = [x.max() for x in hr_ds]\n",
    "        hr_ds = [normalize(x, idx) for idx, x in enumerate(hr_ds)]\n",
    "        lr_ds = [normalize(x, idx) for idx, x in enumerate(lr_ds)]\n",
    "    \n",
    "\n",
    "    hr_ds = tf.cast(hr_ds, tf.float32)\n",
    "    hr_ds = [tf.stack((x,)*3, axis=-1) for x in hr_ds]\n",
    "    hr_ds = tf.convert_to_tensor(hr_ds)\n",
    "\n",
    "    lr_ds = tf.cast(lr_ds, tf.float32)\n",
    "    lr_ds = [tf.stack((x,)*3, axis=-1) for x in lr_ds]\n",
    "    lr_ds = tf.convert_to_tensor(lr_ds)\n",
    "    \n",
    "    print(\"LR Dataset Shape: \", lr_ds.shape)\n",
    "    print(\"HR Dataset Shape: \", hr_ds.shape)\n",
    "    \n",
    "    return lr_ds, hr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c36b6-0134-4aac-bfca-d0340f8bcde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(lr_train_ds, hr_train_ds, repeat=False): \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((lr_train_ds, hr_train_ds))\n",
    "    \n",
    "    if repeat == True: \n",
    "        dataset = dataset.cache().repeat()\n",
    "    else: \n",
    "        # no .repeat() for no infinite dataset\n",
    "        dataset = dataset.cache()\n",
    "\n",
    "    bs = Params['batch_size']\n",
    "    shuffle = True\n",
    "\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "\n",
    "    dataset = (dataset.batch(bs, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3cc9fc-929d-46f9-a987-f1ee5b712e03",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46115b6f-7fae-4c8d-a6a9-def1285aba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, filters, activation=True): \n",
    "    h = layers.Conv2D(filters, kernel_size=[3,3], \n",
    "                      kernel_initializer='he_normal', bias_initializer='zeros', \n",
    "                      strides=[1,1], padding='same', use_bias=True)(input)\n",
    "    if activation: \n",
    "        h = layers.LeakyReLU(0.2)(h)\n",
    "    return h\n",
    "\n",
    "def small_block(input): \n",
    "    h1 = conv_block(input, 32)\n",
    "    h1 = layers.Concatenate()([input, h1])\n",
    "    \n",
    "    h2 = conv_block(h1, 32)\n",
    "    h2 = layers.Concatenate()([h1, h2])\n",
    "    \n",
    "    h3 = conv_block(h2, 32)\n",
    "    h3 = layers.Concatenate()([h2, h3])\n",
    "    \n",
    "    h4 = conv_block(h3, 32)\n",
    "    h4 = layers.Concatenate()([h3, h4])\n",
    "    \n",
    "    h5 = conv_block(h4, 64, activation=False)\n",
    "    \n",
    "    h5 = layers.Lambda(lambda x:x * 0.2)(h5)\n",
    "    h = layers.Add()([h5, input])\n",
    "    \n",
    "    return h\n",
    "\n",
    "def large_block(input): \n",
    "    h = small_block(input)\n",
    "    h = small_block(h)\n",
    "    h = small_block(h)\n",
    "    h = layers.Lambda(lambda x:x *0.2)(h)\n",
    "    out = layers.Add()([h, input])\n",
    "    return out\n",
    "\n",
    "def upsample_block(x, filters):\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    x = conv_block(x, 64, activation=True)\n",
    "    return x\n",
    "    \n",
    "def generator_network(filter=64, trunk_size=Params['trunk_size'], scale=Params['scale'], out_channels=3):\n",
    "    lr_input = layers.Input(shape=(None, None, 3))\n",
    "    x = layers.Conv2D(filter, kernel_size=[3,3], kernel_initializer='he_normal', bias_initializer='zeros', strides=[1,1], padding='same', use_bias=True)(lr_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    ref = x\n",
    "    for i in range(trunk_size): \n",
    "        x = large_block(x)\n",
    "    \n",
    "    x = layers.Conv2D(filter, kernel_size=[3,3], kernel_initializer='he_normal', bias_initializer='zeros', strides=[1,1], padding='same', use_bias=True)(x)\n",
    "    x = layers.Add()([x, ref])\n",
    "    \n",
    "    if scale == 2: \n",
    "         x = upsample_block(x, filter)\n",
    "    if scale == 4: \n",
    "        x = upsample_block(x, filter)\n",
    "        x = upsample_block(x, filter)\n",
    "        \n",
    "    if scale == 8 : \n",
    "        x = upsample_block(x, filter)\n",
    "        x = upsample_block(x, filter)\n",
    "        x = upsample_block(x, filter)\n",
    "    \n",
    "    x = layers.Conv2D(filter, kernel_size=[3,3], kernel_initializer='he_normal', bias_initializer='zeros', strides=[1,1], padding='same', use_bias=True)(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    hr_output = layers.Conv2D(out_channels, kernel_size=[3,3], kernel_initializer='he_normal', bias_initializer='zeros', strides=[1,1], padding='same', use_bias=True)(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=lr_input, outputs=hr_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106b29f-8ef0-4c4b-9136-8145ddb71e5b",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Pre-Training Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce48f6a-e064-4b61-90af-e7192041d9ce",
   "metadata": {},
   "source": [
    "Dataset with degraded LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a181d5-e19d-44e9-8480-6f3e6101b944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_train, hr_train = load_dataset_with_degraded_LR('HR_train', is_val_ds=False)\n",
    "train_dataset = make_dataset(lr_train, hr_train)\n",
    "\n",
    "lr_val, hr_val = load_dataset_with_degraded_LR('HR_test', is_val_ds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421e267-c65f-488c-ab2e-fe37b96af59a",
   "metadata": {},
   "source": [
    "Dataset with real LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d87a7c-bbb7-40d9-b602-f67da48c92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_train, hr_train = load_dataset_with_real_LR('HR_train', 'LR_train', is_val_ds=False)\n",
    "# train_dataset = make_dataset(lr_train, hr_train)\n",
    "\n",
    "# lr_val, hr_val = load_dataset_with_real_LR('HR_test', 'LR_test' is_val_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d94bf5-ca07-46d4-af19-42c6f4c13ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c1af1a-e0a4-4f7e-ab75-4ba5ad7aa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mirrored strategy to use all available GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# compile model\n",
    "with mirrored_strategy.scope():\n",
    "    generator = generator_network()\n",
    "    #generator_200 = tf.keras.models.load_model(Params['model_dir'] + '/256_Generator/256_x4_200epochs_lr5_bs8')\n",
    "    time_callback = TimeHistory()\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=75)\n",
    "    best_model_name = Params['data_name'] +'_'+ str(Params['hr_dimension']) + '_x' + str(Params['scale']) + '_best_model.h5'\n",
    "    mc = ModelCheckpoint(best_model_name, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    \n",
    "    generator.compile(\n",
    "        loss=tf.keras.losses.MeanAbsoluteError(), \n",
    "        optimizer= keras.optimizers.Adam(lr=Params['init_lr'], beta_1 = 0.9, beta_2 = 0.999),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bde2b-52c2-4caf-bfa9-2f50e301cc47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f5b2e-8db8-4a03-b193-8b69742061ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history): \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(Params['data_name'] +' ('+ str(Params['hr_dimension']) + ' x' + str(Params['scale']) + ') model loss')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.savefig('history_plots/'+ Params['data_name'] +'_'+ str(Params['hr_dimension']) + '_x' + str(Params['scale']) + '_model_loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f3088-2baa-4aa3-bd54-7d145f8ea603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_details(history, times):\n",
    "    with open(\"history_plots/training_details\" + name + \"_x\" + Params['scale'] + \".txt\", \"a\") as f:\n",
    "\n",
    "        print(f\"Training on 4 GPUs with tf.mirroredStrategy \\\n",
    "        (batch size {Params['batch_size']/4} per GPU = batch size {Params['batch_size']}). \\\n",
    "        \\nLearning rate: {Params['init_lr']}\", file=f)\n",
    "\n",
    "        print(f\"After epoch {np.argmin(history.history['val_loss'])+1} the minimal validation loss \\\n",
    "        {min(history.history['val_loss']):.4f} was reached.\", file=f)\n",
    "\n",
    "        print(f\"Time in hours for training the model for {np.argmin(history.history['val_loss'])+1} epochs: {(times[1]*(np.argmin(history.history['val_loss'])+1))/60/60} h.\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a035e-6e61-47b6-ae8c-e95f79789726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = generator.fit(train_dataset, \n",
    "                        validation_data=(lr_val, hr_val), \n",
    "                        epochs=1500, \n",
    "                        verbose=2, \n",
    "                        callbacks=[es,mc,time_callback])\n",
    "\n",
    "plot_history(history)\n",
    "print_training_details(history, time_callback.times)\n",
    "model = keras.models.load_model(best_model_name)\n",
    "model.save(model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7106442-1d45-4fb6-ab0d-d9d4600e0223",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df053810-c69b-4d1f-a896-898f63b68cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr_train, hr_train = load_dataset_with_degraded_LR('HR_train', is_val_ds=False)\n",
    "train_dataset = make_dataset(lr_train, hr_train, repeat=True)\n",
    "\n",
    "lr_val, hr_val = load_dataset_with_degraded_LR('HR_test', is_val_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b3ca7-bf8b-4fc4-ad41-548eeb74d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_train, hr_train = load_dataset_with_real_LR('HR_train', 'LR_train', is_val_ds=False)\n",
    "# train_dataset = make_dataset(lr_train, hr_train, repeat=True)\n",
    "\n",
    "# lr_val, hr_val = load_dataset_with_real_LR('HR_test', 'LR_test' is_val_ds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343c3da-d562-4605-9aba-e41a3991941b",
   "metadata": {},
   "source": [
    "#### Dicriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d89a9-0ceb-464f-a845-ec284df9d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block_d(x, out_channel):\n",
    "    x = layers.Conv2D(out_channel, 3,1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(out_channel, 4,2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.8)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def discriminator_network(filters = 64, training=True):\n",
    "    img = layers.Input(shape = (Params['hr_dimension'], Params['hr_dimension'], 3))\n",
    "    \n",
    "    x = layers.Conv2D(filters, [3,3], 1, padding='same', use_bias=False)(img)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, [3,3], 2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = _conv_block_d(x, filters *2)\n",
    "    x = _conv_block_d(x, filters *4)\n",
    "    x = _conv_block_d(x, filters *8)\n",
    "  \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = img, outputs = x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d05cb-76f7-4cb1-9199-9575b6a3f9f8",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c40b4-1881-41b7-81d2-6154c25eef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.reduce_mean(tf.abs(y_true - y_pred), axis = 0))\n",
    "\n",
    "# Function for calculating perceptual loss\n",
    "def vgg_loss(weight=None, input_shape=None):\n",
    "    vgg_model = tf.keras.applications.vgg19.VGG19(\n",
    "        input_shape=input_shape, weights=weight, include_top=False\n",
    "    )\n",
    "\n",
    "    for layer in vgg_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    vgg_model.get_layer(\"block5_conv4\").activation = lambda x: x\n",
    "    vgg = tf.keras.Model(\n",
    "      inputs=[vgg_model.input],\n",
    "      outputs=[vgg_model.get_layer(\"block5_conv4\").output])\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        return tf.compat.v1.losses.absolute_difference(vgg(y_true), vgg(y_pred))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def relativistic_discriminator_loss(discriminator_real_outputs,\n",
    "                                    discriminator_gen_outputs,\n",
    "                                    scope=None):\n",
    "    \"\"\"Relativistic Average GAN discriminator loss.\"\"\"\n",
    "\n",
    "    with tf.compat.v1.name_scope(\n",
    "        scope,\n",
    "        'relativistic_discriminator_loss',\n",
    "        values=[discriminator_real_outputs, discriminator_gen_outputs]):\n",
    "\n",
    "        def get_logits(x, y):\n",
    "              return x - tf.reduce_mean(y)\n",
    "\n",
    "        real_logits = get_logits(discriminator_real_outputs,\n",
    "                                 discriminator_gen_outputs)\n",
    "        gen_logits = get_logits(discriminator_gen_outputs,\n",
    "                                discriminator_real_outputs)\n",
    "\n",
    "        real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.ones_like(real_logits), logits=real_logits))\n",
    "        gen_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.zeros_like(gen_logits), logits=gen_logits))\n",
    "\n",
    "    return real_loss + gen_loss\n",
    "\n",
    "\n",
    "def relativistic_generator_loss(discriminator_real_outputs,\n",
    "                                discriminator_gen_outputs,\n",
    "                                scope=None):\n",
    "    \"\"\"Relativistic Average GAN generator loss.\"\"\"\n",
    "    \n",
    "    with tf.compat.v1.name_scope(\n",
    "        scope,\n",
    "        'relativistic_generator_loss',\n",
    "        values=[discriminator_real_outputs, discriminator_gen_outputs]):\n",
    "\n",
    "        def get_logits(x, y):\n",
    "            return x - tf.reduce_mean(y)\n",
    "        \n",
    "        real_logits = get_logits(discriminator_real_outputs,\n",
    "                                 discriminator_gen_outputs)\n",
    "        gen_logits = get_logits(discriminator_gen_outputs,\n",
    "                                discriminator_real_outputs)\n",
    "        \n",
    "        real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.zeros_like(real_logits), logits=real_logits))\n",
    "        gen_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf.ones_like(gen_logits), logits=gen_logits))\n",
    "\n",
    "    return real_loss + gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe15484-7e01-458b-b007-44828af925f5",
   "metadata": {},
   "source": [
    "#### Training of GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35062289-fe7d-490e-a848-3b5bf9e59b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use mirrored strategy to use all available GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "dist_dataset = mirrored_strategy.experimental_distribute_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c70286-65ba-49c0-9a6a-dbbaa53c2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_model_name = 'models/GAN_Pangaea/Discriminator_Pangaea_256_x8_49000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2820d2-5cca-4dc4-9f2e-354e5feb606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam(\n",
    "        learning_rate = 0.0002, \n",
    "        beta_1 = 0.9,\n",
    "        beta_2 = 0.99)\n",
    "    \n",
    "    \n",
    "    # load pretrained generator\n",
    "    generator = tf.keras.models.load_model(model_name)\n",
    "    \n",
    "    #discriminator = discriminator_network()\n",
    "    discriminator = tf.keras.models.load_model(disc_model_name)\n",
    "    \n",
    "    g_optimizer = optimizer\n",
    "    g_optimizer.learning_rate.assign(Params['disc_init_lr'])\n",
    "    d_optimizer = optimizer\n",
    "    \n",
    "    checkpoint = tf.train.Checkpoint(G=generator,\n",
    "                                 D = discriminator,\n",
    "                                 G_optimizer=g_optimizer,\n",
    "                                 D_optimizer=d_optimizer)\n",
    "   # local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "    \n",
    "    perceptual_loss = vgg_loss(\n",
    "        weight = \"imagenet\",\n",
    "        input_shape = [Params['hr_dimension'], Params['hr_dimension'], 3])\n",
    "    \n",
    "    gen_metric = tf.keras.metrics.Mean()\n",
    "    disc_metric = tf.keras.metrics.Mean()\n",
    "    psnr_metric = tf.keras.metrics.Mean()\n",
    "    metric = tf.keras.metrics.Mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448e606-3c20-4207-9550-e19c52edbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(image_lr, image_hr):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        #image_lr, image_hr = inputs\n",
    "        fake = generator(image_lr)\n",
    "\n",
    "        percep_loss = tf.reduce_mean(perceptual_loss(image_hr, fake))\n",
    "        l1_loss = pixel_loss(image_hr, fake) \n",
    "\n",
    "        real_logits = discriminator(image_hr) \n",
    "        fake_logits = discriminator(fake) \n",
    "\n",
    "        loss_RaG = relativistic_generator_loss(real_logits,\n",
    "                                               fake_logits) \n",
    "        disc_loss = relativistic_discriminator_loss(real_logits,\n",
    "                                                    fake_logits) \n",
    "\n",
    "        gen_loss = percep_loss + Params['lambda'] * loss_RaG + Params['eta'] * l1_loss\n",
    "\n",
    "        gen_loss = gen_loss / 2 #Params['batch_size'] # hier wegen distributed evtl. noch durch anzahl der GPUs teilen\n",
    "        disc_loss = disc_loss / 2 #Params['batch_size']     \n",
    "        psnr_loss = tf.image.psnr(tf.clip_by_value(fake, 0., 1.), tf.clip_by_value(image_hr, 0., 1.), max_val = 1.0)\n",
    "        #rmse_loss = # rmse loss\n",
    "\n",
    "        disc_metric(disc_loss) \n",
    "        gen_metric(gen_loss)\n",
    "        psnr_metric(psnr_loss)\n",
    "\n",
    "        disc_grad = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(disc_grad, discriminator.trainable_variables))\n",
    "\n",
    "        gen_grad = gen_tape.gradient(gen_loss, generator.trainable_variables) \n",
    "        g_optimizer.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
    "\n",
    "    return [disc_loss, gen_loss, psnr_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315f6fa-7c4f-490e-983c-c1bd8131a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_list(l: list):\n",
    "    return [mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, v, axis=None) for v in l]\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(x, y):\n",
    "    results = mirrored_strategy.run(train_step, args=(x, y))\n",
    "    results = reduce_list(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966813b6-f235-4b44-9c2d-1d115bba5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan_history(history, step, label): \n",
    "    plt.plot(history , label = label)\n",
    "    plt.legend()\n",
    "    plt.savefig('history_plots/'+ Params['data_name'] + '/GAN_x'+ str(Params['scale']) + '_' + label + '_' + str(step_count)+ '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce64ea0-ab67-429a-a580-163f757d028f",
   "metadata": {},
   "source": [
    "# **change directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f77df-748f-46ab-87a3-129dea11425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dist_dataset)\n",
    "\n",
    "#step_count = 0\n",
    "#decay_step = [9000, 30000, 50000]\n",
    "\n",
    "step_count = 49000\n",
    "decay_step = [50000]\n",
    "\n",
    "disc_hist = []\n",
    "gen_hist = []\n",
    "psnr_hist = []\n",
    "\n",
    "while step_count < Params['ph2_steps']:\n",
    "\n",
    "    lr, hr = next(iterator)\n",
    "\n",
    "    # if tf.train.latest_checkpoint(Params['ckpt_dir']): \n",
    "    #     checkpoint.restore(tf.train.latest_checkpoint(Params['ckpt_dir']))\n",
    "        \n",
    "    disc_loss, gen_loss, psnr_loss  = distributed_train_step(lr, hr)\n",
    "    \n",
    "    disc_hist.append(disc_loss)\n",
    "    gen_hist.append(gen_loss)\n",
    "    psnr_hist.append(psnr_loss)\n",
    "\n",
    "    if step_count % 1000 == 0:\n",
    "        print(\"step {}\".format(step_count) + \"   Generator Loss = {}   \".format(gen_metric.result()) + \n",
    "              \"Disc Loss = {}\".format(disc_metric.result()) + \"   PSNR : {}\".format(psnr_metric.result()))\n",
    "\n",
    "        \n",
    "        os.makedirs('models/GAN_Pangaea/Generator_' + Params['data_name'] + '_x' + str(Params['scale']) + '_' + str(step_count), exist_ok = True)\n",
    "        os.makedirs('models/GAN_Pangaea/Discriminator_' + Params['data_name'] + '_x' + str(Params['scale']) + '_' + str(step_count), exist_ok = True)\n",
    "\n",
    "        generator.save('models/GAN_Pangaea/Generator_' + Params['data_name'] + '_x' + str(Params['scale']) + '_' + str(step_count))\n",
    "        discriminator.save('models/GAN_Pangaea/Discriminator_' + Params['data_name'] + '_x' + str(Params['scale']) + '_' + str(step_count))\n",
    "        \n",
    "        if step_count != 0 : \n",
    "            plot_gan_history(disc_hist, step_count, 'Discriminator_loss')\n",
    "            plot_gan_history(gen_hist, step_count, 'Generator_loss')\n",
    "            plot_gan_history(psnr_hist, step_count, 'PSNR')\n",
    "\n",
    "    #checkpoint.write(Params['ckpt_dir'], options=local_device_option)\n",
    "\n",
    "    if step_count >= decay_step[0]:\n",
    "        decay_step.pop(0)\n",
    "        g_optimizer.learning_rate.assign(\n",
    "            g_optimizer.learning_rate * Params['decay_ph2'])\n",
    "        d_optimizer.learning_rate.assign(\n",
    "            d_optimizer.learning_rate * Params['decay_ph2'])\n",
    "\n",
    "    step_count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d348278-de57-4eb1-93bc-9d90aaeef5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
